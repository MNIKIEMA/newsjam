{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feaed337",
   "metadata": {},
   "source": [
    "## ONLY if running on Colaboratory, run this cell first (once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/pie3636/newsjam.git\n",
    "!mv newsjam/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23b536",
   "metadata": {},
   "source": [
    "## Install missing modules if needed (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceafccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -r requirements.txt\n",
    "!python -m spacy download fr_core_news_sm\n",
    "# Note: You'll have to restart the kernel/runtime after running this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6422ab",
   "metadata": {},
   "source": [
    "## Imports (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f9c5abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset mlsum (C:\\Users\\maxim\\.cache\\huggingface\\datasets\\mlsum\\fr\\1.0.0\\77f23eb185781f439927ac2569ab1da1083195d8b2dab2b2f6bbe52feb600688)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac58f36cfad4d6eaced29ec30cc8b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at flaubert/flaubert_large_cased were not used when initializing FlaubertModel: ['pred_layer.proj.bias', 'pred_layer.proj.weight']\n",
      "- This IS expected if you are initializing FlaubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at camembert/camembert-large were not used when initializing CamembertModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# MLSUM Corpus\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Loading article data\n",
    "import json\n",
    "\n",
    "# Our packages\n",
    "from eval.rouge_l import RougeLEval\n",
    "from eval.bert_eval import BERT_Eval\n",
    "from eval.time import TimeEval\n",
    "from summ.lsa import LSASummarizer\n",
    "from summ.bert_embed import BertEmbeddingsSummarizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = load_dataset('mlsum', 'fr')\n",
    "\n",
    "rouge_l = RougeLEval()\n",
    "bert = BERT_Eval()\n",
    "timer = TimeEval()\n",
    "lsa_summ = LSASummarizer()\n",
    "flaubert_summ = BertEmbeddingsSummarizer('flaubert/flaubert_large_cased')\n",
    "camembert_summ = BertEmbeddingsSummarizer('camembert/camembert-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e57dfaa",
   "metadata": {},
   "source": [
    "## Summarize a single article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdf0df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pick an article and its reference summary\n",
    "article = dataset['test']['text'][54]\n",
    "ref_summ = dataset['test']['summary'][54]\n",
    "\n",
    "# Computes the summary and evaluation\n",
    "timer.evaluate_one(article, BertEmbeddingsSummarizer, 'camembert/camembert-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b30944c",
   "metadata": {},
   "source": [
    "## Summarize a series of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9cf57c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [03:24<00:00, 20.47s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.630670810000005"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = dataset['test']['text'][:10]\n",
    "ref_summs = dataset['test']['summary'][:10]\n",
    "\n",
    "# Here we pick 5 articles\n",
    "# gen_summs = []\n",
    "# for text in tqdm(texts[:5]):\n",
    "#     gen_summs.append(flaubert_summ.get_summary(text))\n",
    "\n",
    "# scores1, scores2 = rouge_l.evaluate_many(ref_summs, gen_summs, 5)\n",
    "# results = rouge_l.get_results(scores1, scores2)\n",
    "\n",
    "# for k, v in results.items():\n",
    "#     print(k.ljust(25), round(v*100, 3), '%')\n",
    "\n",
    "timer.evaluate_many(texts, LSASummarizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb0d4b",
   "metadata": {},
   "source": [
    "#### Optional: Save generated summaries to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8fa0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated.txt', 'w') as f:\n",
    "    for summ1, summ2 in tqdm(gen_summs):\n",
    "        f.write(summ1)\n",
    "        f.write('\\n\\n')\n",
    "        f.write(summ2)\n",
    "        f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b970a",
   "metadata": {},
   "source": [
    "## Summarize a series of scraped articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e78b51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('data/actu_preliminary.json', 'r', encoding='utf-8') as jsonfile:\n",
    "    data = json.load(jsonfile)\n",
    "\n",
    "texts = [article['text'] for article in data]\n",
    "ref_summs = [article['summary'] for article in data]\n",
    "\n",
    "gen_summs = []\n",
    "for text in tqdm(texts):\n",
    "    gen_summs.append(flaubert_summ.get_summary(text))\n",
    "\n",
    "scores1, scores2 = rouge_l.evaluate_many(ref_summs, gen_summs)\n",
    "results = rouge_l.get_results(scores1, scores2)\n",
    "\n",
    "for k, v in results.items():\n",
    "    print(k.ljust(25), round(v*100, 3), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002dada",
   "metadata": {},
   "source": [
    "Implementation of BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d3342",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_summs, short_summs, ref_summs, key_ref_sums =  bert.split_summs(gen_summs, ref_summs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0d9ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.bert_score(long_summs, short_summs, ref_summs, key_ref_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.get_matrix(long_summs, ref_summs, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
