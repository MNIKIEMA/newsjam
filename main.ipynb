{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feaed337",
   "metadata": {},
   "source": [
    "## ONLY if running on Colaboratory, run this cell first (once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/pie3636/newsjam.git\n",
    "!mv newsjam/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23b536",
   "metadata": {},
   "source": [
    "## Install missing modules if needed (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceafccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -r requirements.txt\n",
    "!python -m spacy download fr_core_news_sm\n",
    "!python -m spacy download en_core_web_sm\n",
    "# Note: You'll have to restart the kernel/runtime after running this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6422ab",
   "metadata": {},
   "source": [
    "## Imports (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f9c5abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset mlsum (/Users/josephkeenan/.cache/huggingface/datasets/mlsum/fr/1.0.0/77f23eb185781f439927ac2569ab1da1083195d8b2dab2b2f6bbe52feb600688)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d986be467447d4a096a2885e7bb69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (/Users/josephkeenan/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82950fa603f7408098c47bd9cd5562c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at flaubert/flaubert_large_cased were not used when initializing FlaubertModel: ['pred_layer.proj.bias', 'pred_layer.proj.weight']\n",
      "- This IS expected if you are initializing FlaubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at camembert/camembert-large were not used when initializing CamembertModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# MLSUM Corpus & CNN/Daily Mail Corpus\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Loading article data\n",
    "import json\n",
    "\n",
    "# Our packages\n",
    "from eval.rouge_l import RougeLEval\n",
    "from eval.bert_eval import BERT_Eval\n",
    "from eval.time import TimeEval\n",
    "\n",
    "from summ.lsa import LSASummarizer\n",
    "from summ.bert_embed import BertEmbeddingsSummarizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_fr = load_dataset('mlsum', 'fr')\n",
    "dataset_en = load_dataset('cnn_dailymail', '3.0.0')\n",
    "\n",
    "rouge_l = RougeLEval()\n",
    "bert = BERT_Eval()\n",
    "timer = TimeEval()\n",
    "lsa_summ = LSASummarizer()\n",
    "flaubert_summ = BertEmbeddingsSummarizer('flaubert/flaubert_large_cased')\n",
    "camembert_summ = BertEmbeddingsSummarizer('camembert/camembert-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e57dfaa",
   "metadata": {},
   "source": [
    "## Summarize a single article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3bdf0df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pick an article and its reference summary\n",
    "article_fr = dataset_fr['test']['text'][54]\n",
    "ref_summ_fr = dataset_fr['test']['summary'][54]\n",
    "\n",
    "article_en = dataset_en['test']['article'][43]\n",
    "ref_summ_en = dataset_en['test']['highlights'][43]\n",
    "\n",
    "# Computes the summary and evaluation\n",
    "# timer.evaluate_one(article, BertEmbeddingsSummarizer, 'camembert/camembert-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b30944c",
   "metadata": {},
   "source": [
    "## Summarize a series of French articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9cf57c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [02:52<00:00,  1.73s/it]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:01<00:00, 86.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long precision avg        14.234 %\n",
      "Long recall avg           17.067 %\n",
      "Long F1-score avg         14.993 %\n",
      "Keyword precision avg     10.225 %\n",
      "Keyword recall avg        13.004 %\n",
      "Keyword F1-score avg      11.046 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts = dataset_fr['test']['text'][:100]\n",
    "ref_summs = dataset_fr['test']['summary'][:100]\n",
    "\n",
    "# Here we pick 50 articles\n",
    "gen_summs = []\n",
    "for text in tqdm(texts[:100]):\n",
    "     gen_summs.append(lsa_summ.get_summary(text))\n",
    "\n",
    "scores1, scores2 = rouge_l.evaluate_many(ref_summs, gen_summs, 100)\n",
    "results = rouge_l.get_results(scores1, scores2)\n",
    "\n",
    "for k, v in results.items():\n",
    "     print(k.ljust(25), round(v*100, 3), '%')\n",
    "\n",
    "#timer.evaluate_many(texts, LSASummarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7fc759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aux descendants de Niki de Saint Phalle, Donald Judd ou Charlotte Perriand échoit la lourde tâche de faire prospérer leurs œuvres. Un métier à temps plein qu’ils exercent au détriment de leurs ambitions personnelles.\n",
      "\n",
      "Autour d’elle, onze autres œuvres de Donald Judd, artiste minimaliste américain mort en 1994.\n",
      "Des projets que Pernette Perriand a accompagnés pendant vingt-sept ans.\n",
      "\n",
      "autour œuvre Donald Judd artiste minimaliste américain mourir 1994\n",
      "projet Pernette Perriand accompagner vingt-sept an\n"
     ]
    }
   ],
   "source": [
    "num = 92\n",
    "\n",
    "print(ref_summs[num])\n",
    "print()\n",
    "print(gen_summs[num][0])\n",
    "print()\n",
    "print(gen_summs[num][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d84b3",
   "metadata": {},
   "source": [
    "## Summarize a series of English articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa2aa8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [02:57<00:00,  1.77s/it]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:01<00:00, 66.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long precision avg        15.911 %\n",
      "Long recall avg           15.948 %\n",
      "Long F1-score avg         15.45 %\n",
      "Keyword precision avg     14.272 %\n",
      "Keyword recall avg        8.644 %\n",
      "Keyword F1-score avg      10.417 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts = dataset_en['test']['article'][:100]\n",
    "ref_summs = dataset_en['test']['highlights'][:100]\n",
    "\n",
    "# Here we pick 50 articles\n",
    "gen_summs = []\n",
    "for text in tqdm(texts[:100]):\n",
    "     gen_summs.append(lsa_summ.get_summary(text, lang='en'))\n",
    "\n",
    "scores1, scores2 = rouge_l.evaluate_many(ref_summs, gen_summs, 100)\n",
    "results = rouge_l.get_results(scores1, scores2)\n",
    "\n",
    "for k, v in results.items():\n",
    "     print(k.ljust(25), round(v*100, 3), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a19c376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marseille, France (CNN)Initial tests on the flight data recorder recovered from downed Germanwings Flight 9525 show that co-pilot Andreas Lubitz purposely used the controls to speed up the plane's descent, according to the French air accident investigation agency, the BEA.\n",
      "\n",
      "French investigators: Flight data recorder reveals Andreas Lubitz acted deliberately to crash plane .\n",
      "He used autopilot to set altitude at 100 feet and then used the controls to speed up the descent .\n"
     ]
    }
   ],
   "source": [
    "num = 35\n",
    "\n",
    "print(ref_summs[num])\n",
    "print()\n",
    "print(gen_summs[num][0])\n",
    "print()\n",
    "print(gen_summs[num][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb0d4b",
   "metadata": {},
   "source": [
    "#### Optional: Save generated summaries to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8fa0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated.txt', 'w') as f:\n",
    "    for summ1, summ2 in tqdm(gen_summs):\n",
    "        f.write(summ1)\n",
    "        f.write('\\n\\n')\n",
    "        f.write(summ2)\n",
    "        f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b970a",
   "metadata": {},
   "source": [
    "## Summarize a series of scraped articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e78b51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|███████████████████████████▍               | 30/47 [00:49<00:28,  1.67s/it]"
     ]
    }
   ],
   "source": [
    "with open('data/actu_preliminary.json', 'r', encoding='utf-8') as jsonfile:\n",
    "    data = json.load(jsonfile)\n",
    "\n",
    "texts = [article['text'] for article in data]\n",
    "ref_summs = [article['summary'] for article in data]\n",
    "\n",
    "gen_summs = []\n",
    "for text in tqdm(texts):\n",
    "    gen_summs.append(lsa_summ.get_summary(text))\n",
    "\n",
    "scores1, scores2 = rouge_l.evaluate_many(ref_summs, gen_summs)\n",
    "results = rouge_l.get_results(scores1, scores2)\n",
    "\n",
    "for k, v in results.items():\n",
    "    print(k.ljust(25), round(v*100, 3), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002dada",
   "metadata": {},
   "source": [
    "## Implementation of BERTScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6113bdc3",
   "metadata": {},
   "source": [
    "Splitting summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "643d3342",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_summs, short_summs, ref_summs, key_ref_summs =  bert.split_summs(gen_summs, ref_summs, gen_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d3c58d",
   "metadata": {},
   "source": [
    "Computation of BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb0d9ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc0e25e3f58404da0594a8add9e3fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61c9656df3049f3ab326a4a2d58b0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 18.72 seconds, 2.51 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e2c1faaefa4c5ea8b0eecbc5b9ea36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bcd77e066447399908fc147ac6b1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 12.90 seconds, 3.64 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Long precision avg': '0.2326',\n",
       " 'Long recall avg': '0.3982',\n",
       " 'Long F1-score avg': '0.3125',\n",
       " 'Keyword precision avg': '0.2440',\n",
       " 'Keyword recall avg': '0.3905',\n",
       " 'Keyword F1-score avg': '0.3151'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.bert_score(long_summs, short_summs, ref_summs, key_ref_summs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07705484",
   "metadata": {},
   "source": [
    "Optional matrix of a score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.get_matrix(long_summs, ref_summs, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475cc9fb",
   "metadata": {},
   "source": [
    "Experimentation w/ BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbfcdac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "test_gen_summ = []\n",
    "test_gen_summ.append('B')\n",
    "print(len(test_gen_summ[0]))\n",
    "\n",
    "print()\n",
    "\n",
    "test_ref_summ = []\n",
    "test_ref_summ.append('汉字')\n",
    "print(len(test_ref_summ[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5c8f562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d160f3f7b3e4dc9a18de0e1ef140e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f03a8b866a6478d8a902c0ce5a23d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 0.13 seconds, 7.84 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Long precision avg': '0.3093',\n",
       " 'Long recall avg': '0.2388',\n",
       " 'Long F1-score avg': '0.2743'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.bert_score(test_gen_summ, test_ref_summ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
