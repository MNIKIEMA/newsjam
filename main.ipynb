{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feaed337",
   "metadata": {},
   "source": [
    "## ONLY if running on Colaboratory, run this cell first (once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/pie3636/newsjam.git\n",
    "!mv newsjam/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23b536",
   "metadata": {},
   "source": [
    "## Install missing modules if needed (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceafccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -r requirements.txt\n",
    "!python -m spacy download fr_core_news_sm\n",
    "# Note: You'll have to restart the kernel/runtime after running this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6422ab",
   "metadata": {},
   "source": [
    "## Imports (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f9c5abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset mlsum (/Users/josephkeenan/.cache/huggingface/datasets/mlsum/fr/1.0.0/77f23eb185781f439927ac2569ab1da1083195d8b2dab2b2f6bbe52feb600688)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d951bc1421d42a0bc2946da91dce6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (/Users/josephkeenan/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0588eb0c18a247a8bf28a5f99482c43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at flaubert/flaubert_large_cased were not used when initializing FlaubertModel: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "- This IS expected if you are initializing FlaubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at camembert/camembert-large were not used when initializing CamembertModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# MLSUM Corpus & CNN/Daily Mail Corpus\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Loading article data\n",
    "import json\n",
    "\n",
    "# Our packages\n",
    "from eval.rouge_l import RougeLEval\n",
    "from eval.bert_eval import BERT_Eval\n",
    "from eval.time import TimeEval\n",
    "\n",
    "from summ.lsa import LSASummarizer\n",
    "from summ.bert_embed import BertEmbeddingsSummarizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_fr = load_dataset('mlsum', 'fr')\n",
    "dataset_en = load_dataset('cnn_dailymail', '3.0.0')\n",
    "\n",
    "rouge_l = RougeLEval()\n",
    "bert = BERT_Eval()\n",
    "timer = TimeEval()\n",
    "lsa_summ = LSASummarizer()\n",
    "flaubert_summ = BertEmbeddingsSummarizer('flaubert/flaubert_large_cased')\n",
    "camembert_summ = BertEmbeddingsSummarizer('camembert/camembert-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e57dfaa",
   "metadata": {},
   "source": [
    "## Summarize a single article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3bdf0df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pick an article and its reference summary\n",
    "article_fr = dataset_fr['test']['text'][54]\n",
    "ref_summ_fr = dataset_fr['test']['summary'][54]\n",
    "\n",
    "article_en = dataset_en['test']['article'][43]\n",
    "ref_summ_en = dataset_en['test']['highlights'][43]\n",
    "\n",
    "# Computes the summary and evaluation\n",
    "# timer.evaluate_one(article, BertEmbeddingsSummarizer, 'camembert/camembert-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b30944c",
   "metadata": {},
   "source": [
    "## Summarize a series of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9cf57c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 50/50 [01:28<00:00,  1.76s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.7803929775000007"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = dataset_fr['test']['text'][:50]\n",
    "ref_summs = dataset_fr['test']['summary'][:50]\n",
    "\n",
    "# Here we pick 5 articles\n",
    "# gen_summs = []\n",
    "# for text in tqdm(texts[:5]):\n",
    "#     gen_summs.append(flaubert_summ.get_summary(text))\n",
    "\n",
    "# scores1, scores2 = rouge_l.evaluate_many(ref_summs, gen_summs, 5)\n",
    "# results = rouge_l.get_results(scores1, scores2)\n",
    "\n",
    "# for k, v in results.items():\n",
    "#     print(k.ljust(25), round(v*100, 3), '%')\n",
    "\n",
    "timer.evaluate_many(texts, LSASummarizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb0d4b",
   "metadata": {},
   "source": [
    "#### Optional: Save generated summaries to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8fa0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated.txt', 'w') as f:\n",
    "    for summ1, summ2 in tqdm(gen_summs):\n",
    "        f.write(summ1)\n",
    "        f.write('\\n\\n')\n",
    "        f.write(summ2)\n",
    "        f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b970a",
   "metadata": {},
   "source": [
    "## Summarize a series of scraped articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e78b51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 47/47 [01:21<00:00,  1.73s/it]\n",
      "100%|███████████████████████████████████████████| 47/47 [00:00<00:00, 95.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long precision avg        49.358 %\n",
      "Long recall avg           61.974 %\n",
      "Long F1-score avg         54.291 %\n",
      "Keyword precision avg     48.302 %\n",
      "Keyword recall avg        61.659 %\n",
      "Keyword F1-score avg      53.503 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/actu_preliminary.json', 'r', encoding='utf-8') as jsonfile:\n",
    "    data = json.load(jsonfile)\n",
    "\n",
    "texts = [article['text'] for article in data]\n",
    "ref_summs = [article['summary'] for article in data]\n",
    "\n",
    "gen_summs = []\n",
    "for text in tqdm(texts):\n",
    "    gen_summs.append(lsa_summ.get_summary(text))\n",
    "\n",
    "scores1, scores2 = rouge_l.evaluate_many(ref_summs, gen_summs)\n",
    "results = rouge_l.get_results(scores1, scores2)\n",
    "\n",
    "for k, v in results.items():\n",
    "    print(k.ljust(25), round(v*100, 3), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002dada",
   "metadata": {},
   "source": [
    "## Implementation of BERTScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6113bdc3",
   "metadata": {},
   "source": [
    "Splitting summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "643d3342",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_summs, short_summs, ref_summs, key_ref_summs =  bert.split_summs(gen_summs, ref_summs, gen_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d3c58d",
   "metadata": {},
   "source": [
    "Computation of BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb0d9ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc0e25e3f58404da0594a8add9e3fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61c9656df3049f3ab326a4a2d58b0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 18.72 seconds, 2.51 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e2c1faaefa4c5ea8b0eecbc5b9ea36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bcd77e066447399908fc147ac6b1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 12.90 seconds, 3.64 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Long precision avg': '0.2326',\n",
       " 'Long recall avg': '0.3982',\n",
       " 'Long F1-score avg': '0.3125',\n",
       " 'Keyword precision avg': '0.2440',\n",
       " 'Keyword recall avg': '0.3905',\n",
       " 'Keyword F1-score avg': '0.3151'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.bert_score(long_summs, short_summs, ref_summs, key_ref_summs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07705484",
   "metadata": {},
   "source": [
    "Optional matrix of a score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.get_matrix(long_summs, ref_summs, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475cc9fb",
   "metadata": {},
   "source": [
    "Experimentation w/ BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbfcdac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "test_gen_summ = []\n",
    "test_gen_summ.append('B')\n",
    "print(len(test_gen_summ[0]))\n",
    "\n",
    "print()\n",
    "\n",
    "test_ref_summ = []\n",
    "test_ref_summ.append('汉字')\n",
    "print(len(test_ref_summ[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5c8f562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d160f3f7b3e4dc9a18de0e1ef140e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f03a8b866a6478d8a902c0ce5a23d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 0.13 seconds, 7.84 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Long precision avg': '0.3093',\n",
       " 'Long recall avg': '0.2388',\n",
       " 'Long F1-score avg': '0.2743'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.bert_score(test_gen_summ, test_ref_summ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
