{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c63af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from datetime import date\n",
    "from datetime import datetime, timedelta\n",
    "import json \n",
    "import locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7abb1a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "front_page = 'https://www.estrepublicain.fr/sante/coronavirus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c791abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_links(source_website):\n",
    "    \"\"\"this function fetches links from the front page\n",
    "    and returns a list\"\"\"\n",
    "    \n",
    "    list_of_links = []\n",
    "    \n",
    "    #pagination on the website follows a simple pattern\n",
    "    #'https://www.estrepublicain.fr/sante/coronavirus?page=NUMBER_OF_PAGE\n",
    "    #thus, we run a loop \n",
    "    \n",
    "    for i in range(101): \n",
    "        url = source_website + '?page=' + str(i)\n",
    "        page = requests.get(url)    \n",
    "        data = page.text\n",
    "        soup = BeautifulSoup(data)\n",
    "        \n",
    "        #catching bugs if a link doesn't contain a substring\n",
    "        for link in soup.find_all('a'):\n",
    "            try:\n",
    "                if link.get('href').startswith('/sante/2'):\n",
    "                    list_of_links.append('https://www.estrepublicain.fr'+link.get('href'))\n",
    "            except AttributeError:\n",
    "                continue\n",
    "    return list_of_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "e4100d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_list = fetch_links('https://www.estrepublicain.fr/sante/coronavirus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "efc47f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.estrepublicain.fr/sante/2021/10/14/covid-19-le-pass-sanitaire-ne-sera-pas-eternel-mais-il-va-etre-prolonge'"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4fb380",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list_of_links.txt', 'w') as f:\n",
    "    for item in big_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7a72b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_info(link):\n",
    "    \"\"\"this function scrapes information\n",
    "    from a single web page, including text, title, and date\"\"\"\n",
    "    \n",
    "    page_single = requests.get(link).text\n",
    "    soup_single = BeautifulSoup(page_single)\n",
    "    \n",
    "    #fetching the date from the link string\n",
    "    date_from_link = link.split('/sante/')[1].split('/')\n",
    "    date_fin = '/'.join(date_from_link[:3])\n",
    "    \n",
    "    #putting together all the text from the page\n",
    "    text = []\n",
    "    for i in soup_single.find_all('p', class_=None):\n",
    "        text.append(i.get_text())\n",
    "    text = text[1:]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    #fetching the title\n",
    "    #two options depending on the page structure\n",
    "    try:\n",
    "        title = soup_single.find('title').get_text()\n",
    "    except:\n",
    "        title = soup_single.find(\"span\",{\"class\":\"headline\"}).next_sibling.lstrip()\n",
    "    \n",
    "    dictionary = {'date': date_fin,\n",
    "               'summary':'',\n",
    "               'text':text,\n",
    "               'title':title,\n",
    "               'topic':'',\n",
    "               'url': link}\n",
    "    return dictionary\n",
    "\n",
    "#json.dumps(dictionary, indent = 6, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d169996",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dic = []\n",
    "with open(\"list_of_links.txt\", \"r\") as a_file:\n",
    "    for line in a_file:\n",
    "        stripped_line = line.strip()\n",
    "        list_of_dic.append(grab_info(stripped_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f483db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure_ascii=False makes sure that french special characters are read correctly\n",
    "\n",
    "with open('lest_republicain.json', 'w') as f:\n",
    "    json.dump(list_of_dic, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
