{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset mlsum (/Users/josephkeenan/.cache/huggingface/datasets/mlsum/fr/1.0.0/77f23eb185781f439927ac2569ab1da1083195d8b2dab2b2f6bbe52feb600688)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a04a3bdb8724a4187db1a9977129f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "dataset = load_dataset('mlsum', 'fr')\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rougeL': Score(precision=0.09090909090909091, recall=0.16666666666666666, fmeasure=0.11764705882352942)}\n",
      "{'rougeL': Score(precision=0.0625, recall=0.14285714285714285, fmeasure=0.08695652173913043)}\n"
     ]
    }
   ],
   "source": [
    "def get_top_sentences(num_topics, top_scores, summary_size=5):\n",
    "    top_sentences = set()\n",
    "    count = 0\n",
    "    for i in range(summary_size):\n",
    "        for j in range(num_topics):\n",
    "            if i >= len(top_scores[j]):\n",
    "                continue\n",
    "            top_sentences.add(top_scores[j][i][0])\n",
    "            if len(top_sentences) == summary_size:\n",
    "                return sorted(top_sentences)\n",
    "    return sorted(top_sentences)\n",
    "\n",
    "def get_summary(article):\n",
    "    doc = nlp(article)\n",
    "\n",
    "    sentences = []\n",
    "    cur_sentence = []\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if not token.text.lower() in STOP_WORDS and not token.is_punct:\n",
    "                cur_sentence.append(token.lemma_)\n",
    "        sentences.append(cur_sentence)\n",
    "        cur_sentence = []\n",
    "\n",
    "    dictionary = corpora.Dictionary(sentences)\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in sentences]\n",
    "    tfidf = models.TfidfModel(doc_term_matrix)\n",
    "    sentences_tfidf = tfidf[doc_term_matrix]\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(2, 10):\n",
    "        model = LsiModel(sentences_tfidf, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=sentences, dictionary=dictionary)\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    max_coherence = coherence_values.index(max(coherence_values))\n",
    "    num_topics = 2 + max_coherence\n",
    "    model = model_list[max_coherence]\n",
    "    corpus_lsi = model[doc_term_matrix]\n",
    "\n",
    "    top_scores = [[] for i in range(num_topics)]\n",
    "    for i, scores in enumerate(corpus_lsi):\n",
    "        for j, score in scores:\n",
    "            top_scores[j].append((i, abs(score)))\n",
    "\n",
    "    for topic in top_scores:\n",
    "        topic.sort(reverse=True, key=lambda x: x[1])\n",
    "        \n",
    "    sents = list(doc.sents)\n",
    "    summary = ''\n",
    "    keyword_summary = ''\n",
    "    added_sents = set()\n",
    "    for i in range(1, len(sents) + 1):\n",
    "        top_sentences = get_top_sentences(num_topics, top_scores, i)\n",
    "        #print(top_sentences)\n",
    "        for sent_idx in top_sentences:\n",
    "            keyword_sent = ' '.join(word for word in sentences[sent_idx])\n",
    "            full_sent = sents[sent_idx].text\n",
    "            if sent_idx not in added_sents and len(summary + full_sent + '\\n') <= 280 + 1:\n",
    "                keyword_summary += keyword_sent + '\\n'\n",
    "                summary += full_sent + '\\n'\n",
    "                added_sents.add(sent_idx)\n",
    "    if summary:\n",
    "        summary = summary[:-1]\n",
    "        keyword_summary = keyword_summary[:-1]\n",
    "    return summary, keyword_summary\n",
    "\n",
    "def evaluate_rouge(summary, long_summ, short_summ):\n",
    "    summ = nlp(summary)\n",
    "    summ_sentences = []\n",
    "    summ_cur_sentence = []\n",
    "    for sent in summ.sents:\n",
    "        for token in sent:\n",
    "            if not token.text.lower() in STOP_WORDS and not token.is_punct:\n",
    "                summ_cur_sentence.append(token.lemma_)\n",
    "        summ_sentences.append(summ_cur_sentence)\n",
    "        summ_cur_sentence = []\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
    "\n",
    "    ref_summary = '\\n'.join([sent.text for sent in summ.sents])\n",
    "    keyword_ref_summary = '\\n'.join([' '.join(sent) for sent in summ_sentences])\n",
    "    \n",
    "    scores = scorer.score(ref_summary, long_summ)\n",
    "    scores_keyword = scorer.score(keyword_ref_summary, short_summ)\n",
    "    return scores, scores_keyword\n",
    "\n",
    "\n",
    "article = dataset['train']['text'][4]\n",
    "summary = dataset['train']['summary'][4]\n",
    "\n",
    "long_summ, short_summ = get_summary(article)\n",
    "scores1, scores2 = evaluate_rouge(summary, long_summ, short_summ)\n",
    "\n",
    "print(scores1)\n",
    "print(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les secours ont été prévenus vers 5 heures du matin, mais \"l'incendie avait déjà bien démarré\", a-t-il expliqué.\n",
      "\"L'origine de l'incendie est indéterminée mais a priori accidentelle\", a déclaré le procureur adjoint de la République de Nîmes, cité par Europe 1.\n"
     ]
    }
   ],
   "source": [
    "print(long_summ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secours prévenir 5 heure matin incendie déjà démarré t il expliquer\n",
      "origine incendie indéterminer priori accidentel déclarer procureur adjoint république Nîmes citer Europe 1\n"
     ]
    }
   ],
   "source": [
    "print(short_summ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cinq personnes sont mortes, et treize autres ont été blessées à Nîmes, dans le Gard, dans un incendie qui s'est déclenché vendredi 1er janvier au petit matin.\n"
     ]
    }
   ],
   "source": [
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('long precision average: 0.2132835301952949',\n",
       " 'keyword precision average: 0.15119047619047618',\n",
       " 'long recall average: 0.3784968229869546',\n",
       " 'keyword recall average: 0.24853801169590642',\n",
       " 'long fmeasure average: 0.2701600965589488',\n",
       " 'keyword fmeasure average: 0.18794326241134754')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_average(article_text, article_sum, num_articles):\n",
    "    long_eval_list = []\n",
    "    keyword_eval_list = []\n",
    "\n",
    "    for x in range(num_articles):\n",
    "        get_summary(article_text[x])\n",
    "        long_summ, short_summ = get_summary(article_text[x])\n",
    "        long_eval, keyword_eval = evaluate_rouge(article_sum[x], long_summ, short_summ)\n",
    "        long_eval_list.append(long_eval)\n",
    "        keyword_eval_list.append(keyword_eval)\n",
    "    \n",
    "    # contains values from long_summ data\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    fmeasure_list = []\n",
    "    \n",
    "    # list2 contains values from the short_summ data\n",
    "    precision_list2 = []\n",
    "    recall_list2 = []\n",
    "    fmeasure_list2 = []\n",
    "    \n",
    "    for x in range(len(long_eval_list)): # Goes through each element in evaluation list (element = rougeL set)\n",
    "        for y in range(3): # Goes through each item within a single rougeL set, precision[0], recall[1], fmeasure[2]\n",
    "            value = long_eval_list[x]['rougeL'][y]\n",
    "            if y == 0:\n",
    "                precision_list.append(value)\n",
    "            if y == 1:\n",
    "                recall_list.append(value)\n",
    "            if y == 2:\n",
    "                fmeasure_list.append(value)\n",
    "                    \n",
    "                    \n",
    "    for x in range(len(keyword_eval_list)):\n",
    "        for y in range(3):\n",
    "            value = keyword_eval_list[x]['rougeL'][y]\n",
    "            if y == 0:\n",
    "                precision_list2.append(value)\n",
    "            if y == 1:\n",
    "                recall_list2.append(value)\n",
    "            if y == 2:\n",
    "                fmeasure_list2.append(value)\n",
    "\n",
    "\n",
    "    list_length = len(precision_list)\n",
    "    \n",
    "    long_precision_avg = sum(precision_list) / list_length\n",
    "    keyword_precision_avg = sum(precision_list2) / list_length\n",
    "    long_recall_avg = sum(recall_list) / list_length\n",
    "    keyword_recall_avg = sum(recall_list2) / list_length\n",
    "    long_fmeasure_avg = sum(fmeasure_list) / list_length\n",
    "    keyword_fmeasure_avg = sum(fmeasure_list2) / list_length\n",
    "    \n",
    "    return f'long precision average: {long_precision_avg}', f'keyword precision average: {keyword_precision_avg}', f'long recall average: {long_recall_avg}', f'keyword recall average: {keyword_recall_avg}', f'long fmeasure average: {long_fmeasure_avg}', f'keyword fmeasure average: {keyword_fmeasure_avg}'\n",
    "\n",
    "article_text = dataset['train']['text']\n",
    "article_sum = dataset['train']['summary']\n",
    "\n",
    "eval_average(article_text, article_sum, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
