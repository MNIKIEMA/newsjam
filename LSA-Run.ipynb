{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset mlsum (C:\\Users\\maxim\\.cache\\huggingface\\datasets\\mlsum\\fr\\1.0.0\\77f23eb185781f439927ac2569ab1da1083195d8b2dab2b2f6bbe52feb600688)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba25d11adfa4bc3815af1add7370a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "dataset = load_dataset('mlsum', 'fr')\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sentences(num_topics, top_scores, summary_size=5):\n",
    "    top_sentences = set()\n",
    "    count = 0\n",
    "    for i in range(summary_size):\n",
    "        for j in range(num_topics):\n",
    "            if i >= len(top_scores[j]):\n",
    "                continue\n",
    "            top_sentences.add(top_scores[j][i][0])\n",
    "            if len(top_sentences) == summary_size:\n",
    "                return sorted(top_sentences)\n",
    "    return sorted(top_sentences)\n",
    "\n",
    "def get_summary(article):\n",
    "    doc = nlp(article)\n",
    "\n",
    "    sentences = []\n",
    "    cur_sentence = []\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if not token.text.lower() in STOP_WORDS and not token.is_punct:\n",
    "                cur_sentence.append(token.lemma_)\n",
    "        sentences.append(cur_sentence)\n",
    "        cur_sentence = []\n",
    "\n",
    "    dictionary = corpora.Dictionary(sentences)\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in sentences]\n",
    "    tfidf = models.TfidfModel(doc_term_matrix)\n",
    "    sentences_tfidf = tfidf[doc_term_matrix]\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(2, 10):\n",
    "        model = LsiModel(sentences_tfidf, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=sentences, dictionary=dictionary)\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    max_coherence = coherence_values.index(max(coherence_values))\n",
    "    num_topics = 2 + max_coherence\n",
    "    model = model_list[max_coherence]\n",
    "    corpus_lsi = model[doc_term_matrix]\n",
    "\n",
    "    top_scores = [[] for i in range(num_topics)]\n",
    "    for i, scores in enumerate(corpus_lsi):\n",
    "        for j, score in scores:\n",
    "            top_scores[j].append((i, abs(score)))\n",
    "\n",
    "    for topic in top_scores:\n",
    "        topic.sort(reverse=True, key=lambda x: x[1])\n",
    "        \n",
    "    sents = list(doc.sents)\n",
    "    summary = ''\n",
    "    keyword_summary = ''\n",
    "    added_sents = set()\n",
    "    for i in range(1, len(sents) + 1):\n",
    "        top_sentences = get_top_sentences(num_topics, top_scores, i)\n",
    "        print(top_sentences)\n",
    "        for sent_idx in top_sentences:\n",
    "            keyword_sent = ' '.join(word for word in sentences[sent_idx])\n",
    "            full_sent = sents[sent_idx].text\n",
    "            if sent_idx not in added_sents and len(summary + full_sent + '\\n') <= 280 + 1:\n",
    "                keyword_summary += keyword_sent + '\\n'\n",
    "                summary += full_sent + '\\n'\n",
    "                added_sents.add(sent_idx)\n",
    "    if summary:\n",
    "        summary = summary[:-1]\n",
    "        keyword_summary = keyword_summary[:-1]\n",
    "    return summary, keyword_summary\n",
    "\n",
    "def evaluate_rouge(summary, long_summ, short_summ):\n",
    "    summ = nlp(summary)\n",
    "    summ_sentences = []\n",
    "    summ_cur_sentence = []\n",
    "    for sent in summ.sents:\n",
    "        for token in sent:\n",
    "            if not token.text.lower() in STOP_WORDS and not token.is_punct:\n",
    "                summ_cur_sentence.append(token.lemma_)\n",
    "        summ_sentences.append(summ_cur_sentence)\n",
    "        summ_cur_sentence = []\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
    "\n",
    "    ref_summary = '\\n'.join([sent.text for sent in summ.sents])\n",
    "    keyword_ref_summary = '\\n'.join([' '.join(sent) for sent in summ_sentences])\n",
    "    \n",
    "    scores = scorer.score(ref_summary, long_summ)\n",
    "    scores_keyword = scorer.score(keyword_ref_summary, short_summ)\n",
    "    return scores, scores_keyword\n",
    "\n",
    "\n",
    "article = dataset['train']['text'][4]\n",
    "summary = dataset['train']['summary'][4]\n",
    "\n",
    "long_summ, short_summ = get_summary(article)\n",
    "scores1, scores2 = evaluate_rouge(summary, long_summ, short_summ)\n",
    "\n",
    "print(scores1)\n",
    "print(scores2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les secours ont été prévenus vers 5 heures du matin, mais \"l'incendie avait déjà bien démarré\", a-t-il expliqué.\n",
      "Les secours ont été prévenus vers 5 heures du matin, mais \"l'incendie avait déjà bien démarré\", a-t-il expliqué.\n"
     ]
    }
   ],
   "source": [
    "print(long_summ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secours être prévenir 5 heure matin incendie bien démarrer -t il expliquer\n",
      "secours être prévenir 5 heure matin incendie bien démarrer -t il expliquer\n"
     ]
    }
   ],
   "source": [
    "print(short_summ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cinq personnes sont mortes, et treize autres ont été blessées à Nîmes, dans le Gard, dans un incendie qui s'est déclenché vendredi 1er janvier au petit matin.\n"
     ]
    }
   ],
   "source": [
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
