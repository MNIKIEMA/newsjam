{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset mlsum (C:\\Users\\maxim\\.cache\\huggingface\\datasets\\mlsum\\fr\\1.0.0\\77f23eb185781f439927ac2569ab1da1083195d8b2dab2b2f6bbe52feb600688)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4db12f8ac345e8b459a189969caa80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MLSUM Corpus\n",
    "from datasets import load_dataset\n",
    "\n",
    "# SpaCy model for segmentation, tokenization, stopwords and stemming\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "# For the first execution, you will need to uncomment this line\n",
    "# to download the SpaCy model. Then you can comment it back\n",
    "# !python -m spacy download fr_core_news_sm\n",
    "\n",
    "# Models for Latent Semantic Indexing\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Evaluation\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Loading article data\n",
    "import json\n",
    "\n",
    "dataset = load_dataset('mlsum', 'fr')\n",
    "nlp = spacy.load(\"fr_core_news_sm\") # Model trained on French News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source functions (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sentences(num_topics, top_scores, article_size):\n",
    "    \"\"\"\n",
    "        Picks the ordered list of indices of the best sentences to summarize the text\n",
    "        Arguments:\n",
    "            `num_topics`   The number of topics used in the LSI model\n",
    "                           Example: 2\n",
    "            `top_scores`   An array containing the top sentences (index and score) for each model\n",
    "                           Example: [[(3, 0.5), (4, 0.35), (1, 0.15)], [(6, 0.75), (1, 0.45), (2, 0.3)]]\n",
    "            `article_size` The number of sentences in the original article\n",
    "        Returns:\n",
    "            A list of the indices of the top sentences\n",
    "    \"\"\"\n",
    "    # Algorithm: First choose the best sentence of each topic\n",
    "    # Then choose the second best sentence of each topic, then the third...\n",
    "    # Keep going until the desired number of sentences has been reached\n",
    "    top_sentences = []\n",
    "    for i in range(article_size):\n",
    "        for j in range(num_topics):\n",
    "            if i >= len(top_scores[j]):\n",
    "                continue\n",
    "            if top_scores[j][i][0] not in top_sentences:\n",
    "                top_sentences.append(top_scores[j][i][0])\n",
    "    return top_sentences\n",
    "\n",
    "def get_summary(article):\n",
    "    \"\"\"\n",
    "        Computes the optimal summary of an article using Latent Semantic Analysis\n",
    "        Arguments:\n",
    "            `article` The raw text content of the original article (without title)\n",
    "        Returns a tuple containing:\n",
    "            - The generated summary in text form\n",
    "            - A keywords-only version of the generated summary\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = nlp(article)\n",
    "\n",
    "    # Split the text into sentences, remove stopwords, stem words and remove punctuation\n",
    "    sentences = []\n",
    "    cur_sentence = []\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if not token.text.lower() in STOP_WORDS and not token.is_punct:\n",
    "                cur_sentence.append(token.lemma_)\n",
    "        sentences.append(cur_sentence)\n",
    "        cur_sentence = []\n",
    "\n",
    "    # Convert sentences to bags of words\n",
    "    dictionary = corpora.Dictionary(sentences)\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in sentences]\n",
    "    \n",
    "    # Create a TF-IDF model that gives each word in each sentence a frequency score\n",
    "    tfidf = models.TfidfModel(doc_term_matrix)\n",
    "    sentences_tfidf = tfidf[doc_term_matrix]\n",
    "\n",
    "    # Try to find the optimal number of topics for Latent Semantic Indexing\n",
    "    # For that, we try using 2, 3, ..., 10 topics and we compute the coherence values\n",
    "    # of the model for each number of topics.\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(2, 10):\n",
    "        model = LsiModel(sentences_tfidf, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=sentences, dictionary=dictionary)\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    # Pick the number of topics that gives the highest coherence values\n",
    "    max_coherence = coherence_values.index(max(coherence_values))\n",
    "    num_topics = 2 + max_coherence\n",
    "    model = model_list[max_coherence]\n",
    "    \n",
    "    # Apply the LSI model to our corpus\n",
    "    corpus_lsi = model[doc_term_matrix]\n",
    "\n",
    "    # Compute and store the scores of each sentence for each topic\n",
    "    top_scores = [[] for i in range(num_topics)]\n",
    "    for i, scores in enumerate(corpus_lsi):\n",
    "        for j, score in scores:\n",
    "            top_scores[j].append((i, abs(score)))\n",
    "\n",
    "    # Sort the tables so that they contain the sentences in decreasing score order\n",
    "    for topic in top_scores:\n",
    "        topic.sort(reverse=True, key=lambda x: x[1])\n",
    "    \n",
    "    # Get a list of all sentences in decreasing order of importance\n",
    "    sents = list(doc.sents)\n",
    "    top_sentences = get_top_sentences(num_topics, top_scores, len(sents) + 1)\n",
    "    \n",
    "    # Try to add each sentence to the summary, starting from the best one\n",
    "    # and making sure to not go over a tweet's length\n",
    "    sents_to_add = []\n",
    "    summary_size = 0\n",
    "    for i in top_sentences:\n",
    "        full_sent = sents[i].text\n",
    "        new_size = summary_size + len(full_sent)\n",
    "        if summary_size + new_size <= 280:\n",
    "            sents_to_add.append(i)\n",
    "            summary_size += len(full_sent) + 1 # +1 because of the space/newline between sentences\n",
    "    \n",
    "    # Now that we have the optimal list of sentences,\n",
    "    # build the actual summary as well as the keyword-only version\n",
    "    summary = ''\n",
    "    keyword_summary = ''\n",
    "    for sent_idx in sents_to_add:\n",
    "        keyword_sent = ' '.join(word for word in sentences[sent_idx])\n",
    "        full_sent = sents[sent_idx].text\n",
    "        keyword_summary += keyword_sent + '\\n'\n",
    "        summary += full_sent + '\\n'\n",
    "    \n",
    "    # Remove the final space/newline\n",
    "    if summary:\n",
    "        summary = summary[:-1]\n",
    "        keyword_summary = keyword_summary[:-1]\n",
    "    return summary, keyword_summary\n",
    "\n",
    "def evaluate_rouge(summary, long_summ, short_summ):\n",
    "    \"\"\"\n",
    "        Computes the ROUGE-L score corresponding to the evaluation of a generated summary\n",
    "        (in two versions: full text and keyword-only version) with a reference one\n",
    "        Arguments:\n",
    "            `summary`    The reference summary of the article\n",
    "            `long_summ`  The generated summary (full text version)\n",
    "            `short_summ` The generated summary (keywords-only version)\n",
    "        Returns a tuple containing:\n",
    "            - The scores of the full text summary\n",
    "            - The scores of the keyword-only summary\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process the reference summary (segment it)\n",
    "    # Also make a copy that is stemmed and has no stopwords to compare it with the\n",
    "    # keyword-only generated summary\n",
    "    summ = nlp(summary)\n",
    "    summ_sentences = []\n",
    "    summ_cur_sentence = []\n",
    "    for sent in summ.sents:\n",
    "        for token in sent:\n",
    "            if not token.text.lower() in STOP_WORDS and not token.is_punct:\n",
    "                summ_cur_sentence.append(token.lemma_)\n",
    "        summ_sentences.append(summ_cur_sentence)\n",
    "        summ_cur_sentence = []\n",
    "\n",
    "    # Creates the instance that allows us to evaluate our summaries\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
    "\n",
    "    # Put the summaries together using newlines (required by RougeScorer)\n",
    "    ref_summary = '\\n'.join([sent.text for sent in summ.sents])\n",
    "    keyword_ref_summary = '\\n'.join([' '.join(sent) for sent in summ_sentences])\n",
    "    \n",
    "    # Compute and return the scores\n",
    "    scores = scorer.score(ref_summary, long_summ)\n",
    "    scores_keyword = scorer.score(keyword_ref_summary, short_summ)\n",
    "    return scores, scores_keyword\n",
    "\n",
    "def sum_loop(article_text, article_sum, num_articles=None):\n",
    "    \"\"\"\n",
    "        Evaluates the summarization process for all articles in a set\n",
    "        Arguments:\n",
    "            `article_text` A list containing the raw text of each article\n",
    "            `article_sum`  A list containing the reference summaries of each article\n",
    "            `num_articles` The number of articles to evaluate (default: all)\n",
    "        Returns a tuple containing:\n",
    "            - The evaluation scores for all full generated summaries\n",
    "            - The evaluation scores for all keywords-only generated summaries\n",
    "    \"\"\"\n",
    "    if num_articles is None:\n",
    "        num_articles = len(article_text)\n",
    "    \n",
    "    long_eval_list = []\n",
    "    keyword_eval_list = []\n",
    "    \n",
    "    for x in tqdm(range(num_articles)):\n",
    "        get_summary(article_text[x])\n",
    "        long_summ, short_summ = get_summary(article_text[x])\n",
    "        long_eval, keyword_eval = evaluate_rouge(article_sum[x], long_summ, short_summ)\n",
    "        long_eval_list.append(long_eval)\n",
    "        keyword_eval_list.append(keyword_eval)\n",
    "    return long_eval_list, keyword_eval_list\n",
    "    \n",
    "def eval_loop(long_eval_list, keyword_eval_list):\n",
    "    \"\"\"\n",
    "        Computes the average evaluation scores from a list\n",
    "        Arguments:\n",
    "            `long_eval_list`    A list containing all evaluation scores for full generated summaries\n",
    "            `keyword_eval_list` A list containing all evaluation scores for keyword-only generated summaries\n",
    "        Returns a dict containing the average precision, recall and f1-score\n",
    "            for both full and keyword_only generated summaries\n",
    "    \"\"\"\n",
    "    # Values from long_summ data\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    fmeasure_list = []\n",
    "    \n",
    "    # Values from short_summ data\n",
    "    precision_list2 = []\n",
    "    recall_list2 = []\n",
    "    fmeasure_list2 = []\n",
    "    \n",
    "    for x in range(len(long_eval_list)): # Goes through each element in evaluation list (element = rougeL set)\n",
    "        for y in range(3): # Goes through each item within a single rougeL set, precision[0], recall[1], fmeasure[2]\n",
    "            value = long_eval_list[x]['rougeL'][y]\n",
    "            if y == 0:\n",
    "                precision_list.append(value)\n",
    "            if y == 1:\n",
    "                recall_list.append(value)\n",
    "            if y == 2:\n",
    "                fmeasure_list.append(value)\n",
    "                    \n",
    "                    \n",
    "    for x in range(len(keyword_eval_list)):\n",
    "        for y in range(3):\n",
    "            value = keyword_eval_list[x]['rougeL'][y]\n",
    "            if y == 0:\n",
    "                precision_list2.append(value)\n",
    "            if y == 1:\n",
    "                recall_list2.append(value)\n",
    "            if y == 2:\n",
    "                fmeasure_list2.append(value)\n",
    "\n",
    "    list_length = len(precision_list)\n",
    "    \n",
    "    results = {}\n",
    "    results[\"Long precision avg\"] = sum(precision_list) / list_length\n",
    "    results[\"Keyword precision avg\"] = sum(precision_list2) / list_length\n",
    "    results[\"Long recall avg\"] = sum(recall_list) / list_length\n",
    "    results[\"Keyword recall avg\"] = sum(recall_list2) / list_length\n",
    "    results[\"Long F1-score avg\"] = sum(fmeasure_list) / list_length\n",
    "    results[\"Keyword F1-score avg\"] = sum(fmeasure_list2) / list_length\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the whole program on a single article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les secours ont été prévenus vers 5 heures du matin, mais \"l'incendie avait déjà bien démarré\", a-t-il expliqué.\n",
      "\n",
      "secours être prévenir 5 heure matin incendie bien démarrer -t il expliquer\n",
      "\n",
      "Cinq personnes sont mortes, et treize autres ont été blessées à Nîmes, dans le Gard, dans un incendie qui s'est déclenché vendredi 1er janvier au petit matin.\n",
      "\n",
      "{'rougeL': Score(precision=0.16666666666666666, recall=0.13333333333333333, fmeasure=0.14814814814814814)}\n",
      "{'rougeL': Score(precision=0.21428571428571427, recall=0.2, fmeasure=0.20689655172413796)}\n"
     ]
    }
   ],
   "source": [
    "# Pick an article and its reference summary\n",
    "article = dataset['train']['text'][4]\n",
    "summary = dataset['train']['summary'][4]\n",
    "\n",
    "# Computes the summary and evaluation\n",
    "long_summ, short_summ = get_summary(article)\n",
    "scores1, scores2 = evaluate_rouge(summary, long_summ, short_summ)\n",
    "print(long_summ)\n",
    "print()\n",
    "print(short_summ)\n",
    "print()\n",
    "print(summary)\n",
    "print()\n",
    "print(scores1)\n",
    "print(scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the whole program on a series of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long precision avg        19.747 %\n",
      "Keyword precision avg     16.677 %\n",
      "Long recall avg           28.736 %\n",
      "Keyword recall avg        24.778 %\n",
      "Long F1-score avg         23.085 %\n",
      "Keyword F1-score avg      19.648 %\n"
     ]
    }
   ],
   "source": [
    "article_text = dataset['test']['text']\n",
    "article_sum = dataset['test']['summary']\n",
    "\n",
    "# Here we pick 5 articles\n",
    "scores1, scores2 = sum_loop(article_text, article_sum, 5)\n",
    "\n",
    "results = eval_loop(scores1, scores2)\n",
    "for k, v in results.items():\n",
    "    print(k.ljust(25), round(v*100, 3), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the whole program on scraped articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('actu-preliminary.json', 'r', encoding='utf-8') as jsonfile:\n",
    "    data = json.load(jsonfile)\n",
    "data_text = [article['text'] for article in data]\n",
    "data_summary = [article['summary'] for article in data]\n",
    "\n",
    "scores1, scores2 = sum_loop(data_text, data_summary)\n",
    "\n",
    "results = eval_loop(scores1, scores2)\n",
    "for k, v in results.items():\n",
    "    print(k.ljust(25), round(v*100, 3), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le masque ne sera plus obligatoire à compter du 18 octobre 2021 dans les stades, les marchés et brocantes de la Loire du moment qu'ils sont en extérieur dans la Loire.\n",
      "\n",
      "masque obligatoire compter 18 octobre 2021 stade marché brocante Loire moment extérieur Loire\n",
      "\n",
      "Le masque ne sera plus obligatoire à compter du 18 octobre 2021 dans les stades, les marchés et brocantes de la Loire du moment qu'ils sont en extérieur dans la Loire.\n",
      "\n",
      "{'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "{'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n"
     ]
    }
   ],
   "source": [
    "# Pick an article and its reference summary\n",
    "article = data_text[1]\n",
    "summary = data_summary[1]\n",
    "\n",
    "# Computes the summary and evaluation\n",
    "long_summ, short_summ = get_summary(article)\n",
    "scores1, scores2 = evaluate_rouge(summary, long_summ, short_summ)\n",
    "print(long_summ)\n",
    "print()\n",
    "print(short_summ)\n",
    "print()\n",
    "print(summary)\n",
    "print()\n",
    "print(scores1)\n",
    "print(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
