{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic steps of the LSA Model:\n",
    "1. Load data\n",
    "2. Preprocess data (via stop word removal and stemming)\n",
    "3. Instantiate TFIDF model, which gives a value to each word based on frequency, but normalizes over all sentences (i.e. if a word is common in one sentence, it is deemed important, but if it’s also common in all sentences, it’s value decreases)\n",
    "4. Calculate coherence values in order to figure out ideal number of topics to split article into\n",
    "5. For each sentence, calculate how relevant that sentence is to each topic (via corpus_lsi model and doc_term_matrix)\n",
    "6. Order sentences from most to least relevant for each topic (via top_scores)\n",
    "7. Going through each topic, picking the top sentence from each topic, and perhaps the second best sentence and so on if needed, until you have reached the summary length limit and sort these sentences according to their original indexes (via get_top_sentences)\n",
    "8. Transform sentences back into original word-forms from the digital forms that we’ve been working with to end up with the summary\n",
    "9. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Tokenization & Stemming\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "# Steps needed to instantiate Latent Semantic Analysis and LSA model itself\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Evaluation\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset mlsum (/Users/josephkeenan/.cache/huggingface/datasets/mlsum/fr/1.0.0/77f23eb185781f439927ac2569ab1da1083195d8b2dab2b2f6bbe52feb600688)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748ac91012674fed9ead83e0cebe170e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading French dataset\n",
    "dataset = load_dataset('mlsum', 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (/Users/josephkeenan/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a428b797221486abe59efd44a9db911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading English dataset\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing an article & reference summary\n",
    "article = dataset['train']['text'][11]\n",
    "# summary = dataset['train']['summary'][11]\n",
    "\n",
    "# This is how you would choose a title (although it is not needed for our program)\n",
    "# title = dataset['train']['title'][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['immeuble', 'jouxter', 'siège', 'social', 'APTN', 'Winnipeg', 'travaille', 'arrache', 'pied', 'transformer', 'intérieur', 'ancien', 'banque', 'studio', 'télévision', 'ultramoderne']\n",
      "\n",
      "['retransmettre', '12', '28', 'février', 'émission', 'consacrer', 'jeu', 'olympique', 'hiver', 'lieu', 'Vancouver', 'Whistler', 'Côte', 'ouest', 'Canada']\n",
      "\n",
      "['aptn', 'diffuseur', 'officiel', 'sein', 'consortium', 'chaîne', 'canadien', 'oeuvrer', 'manière', 'grand', 'partie', 'langue', 'autochtone']\n",
      "\n",
      "['désignation', 'grand', 'réussite', 'réjouir', 'Jean', 'laros', 'directeur', 'général', 'chaîne', 'nommer', 'radiodiffuseur', 'autochtone', 'officiel', 'jeu', 'hiver']\n",
      "\n",
      "['réseau', 'proposer', 'couverture', 'direct', 'haute', 'définition', 'compétition', 'cérémonie', 'reportage', 'émission', 'information', 'olympique']\n",
      "\n",
      "['programmation', 'spécial', 'heure', 'jour', 'structurer', 'bloc', 'linguistique', 'heure', 'anglais', 'autant', 'français', 'couvertur', 'vraiment', 'national', 'français', 'autant', 'langue', 'autochtone', 'particulièrement', 'heure', 'grand', 'écoute']\n",
      "\n",
      "['avoir', 'journaliste', 'sportif', 'innus', 'cris', 'déner', 'explique', 'm.', 'laros']\n",
      "\n",
      "['mettre', 'profit', 'dernier', 'mois', 'former', 'équipe', 'trentaine', 'autochtone', 'langue']\n",
      "\n",
      "['aide', 'journaliste', 'sportif', 'plupart', 'retraiter', 'Radio', 'Canada', 'apprendre', 'subtilité', 'sport', 'terminologie', 'particulier', 'associer']\n",
      "\n",
      "['objectif', 'pouvoir', 'commenter', 'compétition', 'langue', 'autochtone', 'professionnel', 'autant', 'colorer']\n",
      "\n",
      "['fois', 'inuit', 'Cris', 'pouvoir', 'événement', 'envergure', 'langue']\n",
      "\n",
      "['hic']\n",
      "\n",
      "['bien', 'terme', 'sportif', 'luge', 'skeleton', 'exister', 'inuktitut', 'tlingit']\n",
      "\n",
      "['aptn', 'trouver', 'solution', 'limite', 'linguistique', 'rencontrer', 'équipe', 'adresse', 'communauté', 'viser', 'demander', 'exemple', 'aide', 'aîné']\n",
      "\n",
      "['ensemble', 'trouver', 'terme', 'venir', 'enrichir', 'langue']\n",
      "\n",
      "['A.', 'Ps']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nlp() function tokenizes the text\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(article)\n",
    "summ = nlp(summary)\n",
    "\n",
    "en = spacy.load('en_core_web_sm')\n",
    "en_stopwords = en.Defaults.stop_words\n",
    "\n",
    "# sentences is a list that contains each tokenized and stemmed sentence\n",
    "sentences = []\n",
    "\n",
    "# cur_sentence is used to put the stemmed sentence into a list, so we can then transfer it to our main sentences list\n",
    "cur_sentence = []\n",
    "\n",
    "# Loop is used to get rid of stop words, then to create a list of all of the lemmas for the remaining words\n",
    "for sent in doc.sents: # For each sentence in the article\n",
    "    for token in sent: # For each token in the sentence\n",
    "        if not token.text.lower() in STOP_WORDS and not token.is_punct:\n",
    "            cur_sentence.append(token.lemma_)\n",
    "    sentences.append(cur_sentence)\n",
    "    cur_sentence = []\n",
    "\n",
    "for s in sentences:\n",
    "    print(s)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(136 unique tokens: ['APTN', 'Winnipeg', 'ancien', 'arrache', 'banque']...)\n",
      "[(0, 0.25), (1, 0.25), (2, 0.25), (3, 0.25), (4, 0.25), (5, 0.25), (6, 0.25), (7, 0.25), (8, 0.25), (9, 0.25), (10, 0.25), (11, 0.25), (12, 0.25), (13, 0.25), (14, 0.25), (15, 0.25)]\n",
      "[(16, 0.27937211830783126), (17, 0.27937211830783126), (18, 0.20952908873087345), (19, 0.27937211830783126), (20, 0.27937211830783126), (21, 0.27937211830783126), (22, 0.27937211830783126), (23, 0.27937211830783126), (24, 0.20952908873087345), (25, 0.20952908873087345), (26, 0.27937211830783126), (27, 0.20952908873087345), (28, 0.27937211830783126), (29, 0.27937211830783126), (30, 0.20952908873087345)]\n",
      "[(31, 0.24523470628015032), (32, 0.13717382358909), (33, 0.32697960837353374), (34, 0.24523470628015032), (35, 0.32697960837353374), (36, 0.32697960837353374), (37, 0.19741700393039868), (38, 0.11567210183701522), (39, 0.32697960837353374), (40, 0.32697960837353374), (41, 0.24523470628015032), (42, 0.32697960837353374), (43, 0.32697960837353374)]\n",
      "[(24, 0.2225899481185063), (25, 0.2225899481185063), (32, 0.12450731276604836), (34, 0.2225899481185063), (37, 0.17918769055624167), (41, 0.2225899481185063), (44, 0.29678659749134173), (45, 0.29678659749134173), (46, 0.29678659749134173), (47, 0.29678659749134173), (48, 0.2225899481185063), (49, 0.29678659749134173), (50, 0.29678659749134173), (51, 0.29678659749134173), (52, 0.29678659749134173)]\n",
      "[(27, 0.22941573387056177), (30, 0.22941573387056177), (53, 0.22941573387056177), (54, 0.305887645160749), (55, 0.305887645160749), (56, 0.305887645160749), (57, 0.305887645160749), (58, 0.305887645160749), (59, 0.305887645160749), (60, 0.305887645160749), (61, 0.305887645160749), (62, 0.305887645160749)]\n",
      "[(32, 0.08003071562978066), (37, 0.11517812720133012), (38, 0.06748606145256905), (63, 0.19076826299504426), (64, 0.2861523944925664), (65, 0.19076826299504426), (66, 0.19076826299504426), (67, 0.3815365259900885), (68, 0.5723047889851328), (69, 0.19076826299504426), (70, 0.1430761972462832), (71, 0.19076826299504426), (72, 0.19076826299504426), (73, 0.19076826299504426), (74, 0.19076826299504426), (75, 0.19076826299504426), (76, 0.19076826299504426), (77, 0.19076826299504426)]\n",
      "[(48, 0.274052718774899), (78, 0.3654036250331987), (79, 0.3654036250331987), (80, 0.3654036250331987), (81, 0.3654036250331987), (82, 0.3654036250331987), (83, 0.274052718774899), (84, 0.3654036250331987), (85, 0.2206158642069001)]\n",
      "[(32, 0.16013021775085062), (38, 0.1350301272518658), (86, 0.3817004915295382), (87, 0.3817004915295382), (88, 0.3817004915295382), (89, 0.3817004915295382), (90, 0.3817004915295382), (91, 0.3817004915295382), (92, 0.28627536864715364)]\n",
      "[(18, 0.22560063958758883), (83, 0.22560063958758883), (85, 0.18161133482177277), (93, 0.30080085278345176), (94, 0.22560063958758883), (95, 0.30080085278345176), (96, 0.30080085278345176), (97, 0.30080085278345176), (98, 0.30080085278345176), (99, 0.30080085278345176), (100, 0.30080085278345176), (101, 0.30080085278345176), (102, 0.30080085278345176)]\n",
      "[(32, 0.1714298457483137), (38, 0.14455856122158778), (53, 0.3064764600837651), (64, 0.3064764600837651), (103, 0.4086352801116868), (104, 0.4086352801116868), (105, 0.4086352801116868), (106, 0.3064764600837651), (107, 0.4086352801116868)]\n",
      "[(38, 0.14833433547897054), (106, 0.31448142304617505), (108, 0.4193085640615667), (109, 0.4193085640615667), (110, 0.4193085640615667), (111, 0.4193085640615667), (112, 0.4193085640615667)]\n",
      "[(113, 1.0)]\n",
      "[(85, 0.2293984598434494), (114, 0.37995014141511296), (115, 0.37995014141511296), (116, 0.37995014141511296), (117, 0.37995014141511296), (118, 0.37995014141511296), (119, 0.2849626060613347), (120, 0.37995014141511296)]\n",
      "[(31, 0.21821789023599236), (70, 0.21821789023599236), (92, 0.21821789023599236), (94, 0.21821789023599236), (121, 0.29095718698132317), (122, 0.29095718698132317), (123, 0.29095718698132317), (124, 0.29095718698132317), (125, 0.29095718698132317), (126, 0.29095718698132317), (127, 0.29095718698132317), (128, 0.29095718698132317), (129, 0.21821789023599236), (130, 0.29095718698132317)]\n",
      "[(38, 0.17159556093956804), (119, 0.36379720189822445), (129, 0.36379720189822445), (131, 0.4850629358642992), (132, 0.4850629358642992), (133, 0.4850629358642992)]\n",
      "[(134, 0.7071067811865475), (135, 0.7071067811865475)]\n"
     ]
    }
   ],
   "source": [
    "# Creation of the TF-IDF scores for each word\n",
    "# Essentially calculates a value for each word based on it's frequency in each sentence \n",
    "## and also takes into account frequency of word in document overall\n",
    "\n",
    "# Converts our preproccesed sentences into a bag of words\n",
    "dictionary = corpora.Dictionary(sentences)\n",
    "print(dictionary)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in sentences]\n",
    "\n",
    "# Instantiate TF-IDF Model\n",
    "tfidf = models.TfidfModel(doc_term_matrix)\n",
    "sentences_tfidf = tfidf[doc_term_matrix]\n",
    "for sent in sentences_tfidf:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2341532253965724, 0.39774293065833694, 0.40715853269905955, 0.40793151643258607, 0.3815894394757713, 0.3781185300425466, 0.3719331925296573, 0.41920108542439904]\n",
      "optimal number of topics: 9\n"
     ]
    }
   ],
   "source": [
    "# Calculation of coherence values, which tells us the optimal number of topics to separate article into\n",
    "\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "\n",
    "# The range indicates the number of topics we are considering; here we are considering between 2 and 9 topics\n",
    "for num_topics in range(2, 10): \n",
    "    model = LsiModel(sentences_tfidf, num_topics=num_topics, id2word=dictionary)\n",
    "    model_list.append(model)\n",
    "    coherencemodel = CoherenceModel(model=model, texts=sentences, dictionary=dictionary)\n",
    "    coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "print(coherence_values)\n",
    "\n",
    "\n",
    "max_coherence = coherence_values.index(max(coherence_values))\n",
    "num_topics = 2 + max_coherence\n",
    "model = model_list[max_coherence]\n",
    "print(f'optimal number of topics: {num_topics}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '-0.217*\"autochtone\" + -0.210*\"langue\" + -0.188*\"autant\" + -0.175*\"officiel\" + -0.175*\"chaîne\" + -0.173*\"grand\" + -0.173*\"pouvoir\" + -0.160*\"heure\" + -0.144*\"professionnel\" + -0.144*\"colorer\"')\n",
      "(1, '0.249*\"sportif\" + 0.217*\"journaliste\" + -0.206*\"pouvoir\" + -0.184*\"autant\" + 0.157*\"déner\" + 0.157*\"innus\" + 0.157*\"explique\" + 0.157*\"m.\" + 0.157*\"avoir\" + 0.157*\"cris\"')\n",
      "(2, '-0.265*\"terme\" + -0.228*\"trouver\" + -0.221*\"ensemble\" + -0.221*\"venir\" + -0.221*\"enrichir\" + 0.172*\"jeu\" + 0.172*\"hiver\" + 0.138*\"chaîne\" + 0.138*\"officiel\" + -0.132*\"exister\"')\n",
      "(3, '-0.206*\"compétition\" + -0.181*\"olympique\" + -0.181*\"émission\" + -0.166*\"pouvoir\" + 0.164*\"chaîne\" + 0.164*\"officiel\" + 0.152*\"aptn\" + -0.152*\"direct\" + -0.152*\"haute\" + -0.152*\"cérémonie\"')\n",
      "(4, '-0.195*\"trouver\" + 0.181*\"journaliste\" + 0.171*\"m.\" + 0.171*\"avoir\" + 0.171*\"explique\" + 0.171*\"innus\" + 0.171*\"déner\" + 0.171*\"cris\" + -0.165*\"émission\" + -0.165*\"olympique\"')\n",
      "(5, '0.256*\"terme\" + -0.209*\"équipe\" + -0.192*\"aide\" + 0.183*\"tlingit\" + 0.183*\"luge\" + 0.183*\"inuktitut\" + 0.183*\"exister\" + 0.183*\"bien\" + 0.183*\"skeleton\" + 0.159*\"ensemble\"')\n",
      "(6, '0.882*\"hic\" + -0.333*\"Ps\" + -0.333*\"A.\" + 0.000*\"heure\" + 0.000*\"français\" + -0.000*\"inuit\" + -0.000*\"événement\" + -0.000*\"fois\" + -0.000*\"Cris\" + -0.000*\"envergure\"')\n",
      "(7, '-0.624*\"Ps\" + -0.624*\"A.\" + -0.471*\"hic\" + -0.000*\"heure\" + 0.000*\"événement\" + 0.000*\"inuit\" + 0.000*\"fois\" + 0.000*\"envergure\" + 0.000*\"Cris\" + -0.000*\"français\"')\n",
      "(8, '0.250*\"immeuble\" + 0.250*\"travaille\" + 0.250*\"jouxter\" + 0.250*\"social\" + 0.250*\"transformer\" + 0.250*\"banque\" + 0.250*\"ancien\" + 0.250*\"arrache\" + 0.250*\"télévision\" + 0.250*\"ultramoderne\"')\n",
      "\n",
      "[(8, 4.0)] Dans l'immeuble jouxtant le siège social d'APTN, à Winnipeg, on travaille d'arrache-pied à transformer l'intérieur d'une ancienne banque en deuxième studio de télévision ultramoderne.\n",
      "[(0, -0.963039631461568), (1, 0.82386204556284), (2, 1.7378001730698796), (3, -1.388651062629509), (4, -1.5091039617040714), (5, -0.0760651315369088)] C'est de là que seront retransmises, du 12 au 28 février, les émissions consacrées aux Jeux olympiques d'hiver qui auront lieu à Vancouver et Whistler, sur la Côte ouest du Canada.\n",
      "[(0, -1.9701368568948596), (1, -0.11506614842584095), (2, 0.706338004645431), (3, 1.6467866786292613), (4, 0.06775720607526682), (5, 0.5274493729743738)] APTN en est, en effet, l'un des diffuseurs officiels, au sein d'un consortium de douze chaînes canadiennes, mais il oeuvrera à sa manière, c'est-à-dire en grande partie en langues autochtones.\n",
      "[(0, -1.9709050612584753), (1, 0.6350294111684694), (2, 1.8256097565498657), (3, 1.1614658246951377), (4, 0.26485022775485934), (5, 0.9229367880973443)] \"Cette désignation est l'une de nos plus grandes réussites\", se réjouit Jean LaRose, directeur général de cette chaîne nommée \"premier radiodiffuseur autochtone officiel\" des Jeux d'hiver.\n",
      "[(0, -0.6799761987732817), (1, -0.2468686836549702), (2, 0.8527225119298094), (3, -1.930835020771919), (4, -1.4148235302700005), (5, -0.1442296986060923)] Le réseau proposera une couverture, en direct et en haute définition, des compétitions et cérémonies, ainsi que des reportages et des émissions d'information \"olympique\".\n",
      "[(0, -2.3486387558314368), (1, -1.6405696000915375), (2, -0.448639404981516), (3, 0.07344295544805707), (4, 1.110992767031645), (5, -0.6151900470749692)] Cette programmation spéciale, de quatorze à seize heures par jour, sera structurée en trois blocs \"linguistiques\" : trois à quatre heures en anglais, autant en français (avec la seule couverture vraiment nationale en français) et autant dans... huit langues autochtones, particulièrement aux heures de grande écoute.\n",
      "[(0, -0.5758735908260145), (1, 1.5627199313329734), (2, -0.005519664097152099), (3, -0.4550313938850119), (4, 1.4868166099243338), (5, -0.22135723322226428)] \"Nous n'avions pas de journalistes sportifs innus, cris ou dénés, explique M. LaRose.\n",
      "[(0, -1.0618172348743244), (1, -0.42113143690695415), (2, -0.5483644501449616), (3, 0.5114456217492115), (4, -0.39700054451513156), (5, -1.057996459982488)] Nous avons donc mis à profit les six derniers mois pour former une équipe d'une trentaine d'autochtones de différentes langues.\n",
      "[(0, -0.6490129885776822), (1, 1.9074609784396943), (2, -0.3073277984932239), (3, -0.9208663181418356), (4, 0.9056538128907963), (5, -1.4686540204494165)] Avec l'aide de journalistes sportifs, pour la plupart retraités de Radio Canada, ils ont appris les subtilités de chaque sport et la terminologie particulière qui y est associée.\n",
      "[(0, -1.507492871895862), (1, -1.332512855167962), (2, -0.352715631083069), (3, -0.8957200718799146), (4, 0.6040023853569226), (5, 0.03838690685975425)] L'objectif est qu'ils puissent commenter les compétitions, dans une ou plusieurs langues autochtones, et ce de façon professionnelle autant que colorée.\n",
      "[(0, -0.813618977865164), (1, -0.9061529315513708), (2, -0.5385634142553991), (3, -0.6639179356570326), (4, 0.6968830856181883), (5, 0.32732214231638534)] \" Pour la première fois, des Inuits ou des Cris pourront ainsi suivre un événement de cette envergure dans leur langue !\n",
      "[(6, 0.8823529411764712), (7, -0.47058823529411675)] Le hic ?\n",
      "[(0, -0.4444332807679881), (1, 1.1301904745697517), (2, -1.1462400267360686), (3, -0.45264856951496146), (4, -0.07337381564111786), (5, 1.3722910714669723)] Bien des termes sportifs comme luge ou skeleton, n'existent pas en inuktitut ou en tlingit...\n",
      "[(0, -1.1292213789704753), (1, 0.4827136721165646), (2, -1.248649441717083), (3, 0.9100228040915429), (4, -1.294804307187052), (5, -1.7784657981773868)] APTN a trouvé la solution : à chaque \"limite\" linguistique rencontrée, l'équipe s'adresse à la communauté visée pour demander, par exemple, l'aide des aînés.\n",
      "[(0, -0.7309935140843008), (1, 0.2857879205903223), (2, -1.2753357613267495), (3, 0.03452018115223495), (4, -0.8191648892530538), (5, 0.7866031785560685)] Ensemble, ils trouvent alors un nouveau terme qui viendra \"enrichir\" cette langue.\n",
      "[(6, -0.6655122646461618), (7, -1.247835496211555)] A. Ps\n"
     ]
    }
   ],
   "source": [
    "# prints each topic, which is essentially a set of words and a number indicating their relation to the topic\n",
    "for topic in model.print_topics(num_topics=num_topics):\n",
    "    print(topic)\n",
    "print()\n",
    "\n",
    "\n",
    "corpus_lsi = model[doc_term_matrix]\n",
    "\n",
    "# This loop prints the values of our corpus_lsi model\n",
    "## text = each sentence\n",
    "### score = the values indicating how much the sentence relates to each topic\n",
    "#### Some sentences have no relation to a topic, which is why some sentences are missing a score for certain topics\n",
    "for score, text in zip(corpus_lsi, doc.sents):\n",
    "    print(score, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[(5, 2.356155775860713), (0, 1.8987248960778664), (2, 1.49967951720882), (4, 1.2681047078648622), (8, 1.1445606469371634), (1, 0.22053925371658237)]\n",
      "[(8, 1.9257494885437403), (0, 1.4604938317813678), (4, 1.4188621942924309), (2, 1.0774367414658732), (1, 0.9888033474091669), (5, 0.9812048869645829)]\n",
      "[(2, 2.2611627453875016), (1, 1.6561184541998684), (8, 1.48756539981413), (0, 0.9734427754384644), (4, 0.6630774409809027), (5, 0.22518166721222574)]\n",
      "[(3, 1.7936007796425586), (7, 0.7200911509287125), (6, 0.30338557593800464)]\n",
      "[(7, 0.9369718039156227), (6, 0.8951026397998952), (3, 0.30468365205031966)]\n",
      "[(7, 1.8983025504437556), (3, 0.8307611663040635), (6, 0.32672382303104996)]\n"
     ]
    }
   ],
   "source": [
    "# Sorts top sentence indices for each topic\n",
    "\n",
    "top_scores = [[] for i in range(num_topics)]\n",
    "\n",
    "# This loop helps us find the top scores\n",
    "# i = sentence index in corpus\n",
    "# j = topic index\n",
    "for i, scores in enumerate(corpus_lsi): \n",
    "    for j, score in scores: \n",
    "        top_scores[j].append((i, abs(score)))\n",
    "\n",
    "# This loop sorts the top scores in descending order for each topic\n",
    "for topic in top_scores:\n",
    "    topic.sort(reverse=True, key=lambda x: x[1])\n",
    "\n",
    "print()\n",
    "for v in top_scores:\n",
    "    print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "# This function takes a summary size (i.e. how many sentences we want in our summary)\n",
    "# Then it takes the top score from each topic and inserts the sentence index into our list 'top_sentences'\n",
    "\n",
    "def get_top_sentences(summary_size=5):\n",
    "    top_sentences = set()\n",
    "    count = 0\n",
    "    for i in range(summary_size): \n",
    "        for j in range(num_topics):\n",
    "            if i >= len(top_scores[j]):\n",
    "                continue\n",
    "            top_sentences.add(top_scores[j][i][0])\n",
    "            if len(top_sentences) == summary_size:\n",
    "                return sorted(top_sentences)\n",
    "            \n",
    "top_sentences = get_top_sentences()\n",
    "print(top_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "secours prévenir 5 heure matin incendie déjà démarré t il expliquer\n",
      "origine incendie indéterminer priori accidentel déclarer procureur adjoint république Nîmes citer Europe 1\n"
     ]
    }
   ],
   "source": [
    "# This block converts the top_sentences indicies back into their actual sentences and returns a summary that is 280 words or less\n",
    "# Thus all of the top_sentences may not always be used\n",
    "\n",
    "sents = list(doc.sents)\n",
    "longest_summary = ''\n",
    "for i in range(1, len(sents) + 1): # i = summary_size (i.e. number of sentences)\n",
    "    top_sentences = get_top_sentences(i)\n",
    "    summary = \"\"\n",
    "    \n",
    "    # joins words back together into strings and adds a new line between each sentence (this new line format was necessary to use the ROUGE evaluation)\n",
    "    for sent_idx in top_sentences: \n",
    "        #summary += sents[sent_idx].text + \"\\n\"\n",
    "        summary += ' '.join(word for word in sentences[sent_idx]) + \"\\n\"\n",
    "    if len(summary) > 280:\n",
    "        break\n",
    "    longest_summary = summary\n",
    "if longest_summary: # Omits last character in summary, which would be a new line ('\\n')\n",
    "    longest_summary = longest_summary[:-1]\n",
    "\n",
    "print(longest_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['personne', 'mourir', 'blesser', 'Nîmes', 'Gard', 'incendie', 'déclencher', 'vendredi', 'premier', 'janvier', 'petit', 'matin']]\n",
      "personne mourir blesser Nîmes Gard incendie déclencher vendredi premier janvier petit matin\n",
      "{'rougeL': Score(precision=0.7741935483870968, recall=0.13114754098360656, fmeasure=0.22429906542056074)}\n",
      "{'rougeL': Score(precision=0.06451612903225806, recall=0.14285714285714285, fmeasure=0.08888888888888889)}\n"
     ]
    }
   ],
   "source": [
    "# ROUGE Evaluation\n",
    "\n",
    "summ_sentences = []\n",
    "summ_cur_sentence = []\n",
    "\n",
    "# This loop takes the words in the reference summary (from the corpus) to remove stop words and stem the remaining keywords\n",
    "for sent in summ.sents: # For each sentence in the summary\n",
    "    for token in sent: # For each word in a sentence\n",
    "        if not token.text.lower() in STOP_WORDS and not token.is_punct:\n",
    "            summ_cur_sentence.append(token.lemma_)\n",
    "    summ_sentences.append(summ_cur_sentence)\n",
    "    summ_cur_sentence = []\n",
    "\n",
    "# Instantiate ROUGE evaluation\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
    "\n",
    "# joins elements in original article into a string from a list\n",
    "original_article = '\\n'.join([sent.text for sent in doc.sents])\n",
    "scores_original = scorer.score(original_article, longest_summary)\n",
    "\n",
    "# joins elements in reference summary into a string from a list\n",
    "#reference_summary = '\\n'.join([sent.text for sent in summ.sents])\n",
    "reference_summary = '\\n'.join([' '.join(sent) for sent in summ_sentences])\n",
    "print(summ_sentences)\n",
    "print(reference_summary)\n",
    "\n",
    "# The ROUGE score evaluates the reference summary against our generated summary (longest_summary)\n",
    "# For this test we evaluated the lemmas in the reference to the lemmas in our generated topic\n",
    "## In our more compact LSA-Run file we compare the scores when we evaluate lemmas vs when we evaluate full sentences (un-stemmed & stop words included)\n",
    "scores_reference = scorer.score(reference_summary, longest_summary)\n",
    "\n",
    "#print()\n",
    "#print(longest_summary)\n",
    "#print()\n",
    "#print(original_article)\n",
    "#print()\n",
    "#print(reference_summary)\n",
    "#print()\n",
    "print(scores_original)\n",
    "#print()\n",
    "print(scores_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
