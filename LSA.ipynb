{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic steps of the LSA Model:\n",
    "1. Load data\n",
    "2. Preprocess data (via stop word removal and stemming)\n",
    "3. Instantiate TFIDF model, which gives a value to each word based on frequency, but normalizes over all sentences (i.e. if a word is common in one sentence, it is deemed important, but if it’s also common in all sentences, it’s value decreases)\n",
    "4. Calculate coherence values in order to figure out ideal number of topics to split article into\n",
    "5. For each sentence, calculate how relevant that sentence is to each topic (via corpus_lsi model and doc_term_matrix)\n",
    "6. Order sentences from most to least relevant for each topic (via top_scores)\n",
    "7. Going through each topic, picking the top sentence from each topic, and perhaps the second best sentence and so on if needed, until you have reached the summary length limit and sort these sentences according to their original indexes (via get_top_sentences)\n",
    "8. Transform sentences back into original word-forms from the digital forms that we’ve been working with to end up with the summary\n",
    "9. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset mlsum (/Users/josephkeenan/.cache/huggingface/datasets/mlsum/fr/1.0.0/77f23eb185781f439927ac2569ab1da1083195d8b2dab2b2f6bbe52feb600688)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c0c2b7f37d4740ab705db38c0297a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('mlsum', 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = dataset['train']['text'][4]\n",
    "summary = dataset['train']['summary'][4]\n",
    "title = dataset['train']['title'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['personne', 'mourir', 'blesser', 'Nîmes', 'Gard', 'incendie', 'déclencher', 'vendredi', 'premier', 'janvier', 'petit', 'matin']\n",
      "\n",
      "['feu', 'ignore', 'origine', 'instant', 'prendre', 'étage', 'immeuble']\n",
      "\n",
      "['arrivée', 'pompier', 'personne', 'décéder', 'appartement', 'appartement', 'voisin', 'intoxication', 'expliquer', 'Télé', 'directeur', 'cabinet', 'préfet', 'Gard']\n",
      "\n",
      "['dénombre', 'également', 'blessé', 'grave']\n",
      "\n",
      "['coma', 'transférer', 'Marseille', 't', 'il', 'ajouter']\n",
      "\n",
      "['secours', 'prévenir', '5', 'heure', 'matin', 'incendie', 'déjà', 'démarré', 't', 'il', 'expliquer']\n",
      "\n",
      "['France']\n",
      "\n",
      "['Info', 'précise', 'victime', 'adulte', 'enfant']\n",
      "\n",
      "['origine', 'incendie', 'indéterminer', 'priori', 'accidentel', 'déclarer', 'procureur', 'adjoint', 'république', 'Nîmes', 'citer', 'Europe', '1']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(article)\n",
    "summ = nlp(summary)\n",
    "\n",
    "sentences = []\n",
    "cur_sentence = []\n",
    "\n",
    "# This loop is used to get rid of stop words, then to create a list of all of the lemmas for the remaining words\n",
    "for sent in doc.sents: # For each sentence in the article\n",
    "    for token in sent: # For each token in the sentence\n",
    "        if not token.text.lower() in STOP_WORDS and not token.is_punct:\n",
    "            cur_sentence.append(token.lemma_)\n",
    "    sentences.append(cur_sentence)\n",
    "    cur_sentence = []\n",
    "\n",
    "for s in sentences:\n",
    "    print(s)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(62 unique tokens: ['Gard', 'Nîmes', 'blesser', 'déclencher', 'incendie']...)\n",
      "[(0, 0.22661814957458093), (1, 0.22661814957458093), (2, 0.3310540860350354), (3, 0.3310540860350354), (4, 0.1655270430175177), (5, 0.3310540860350354), (6, 0.22661814957458093), (7, 0.3310540860350354), (8, 0.22661814957458093), (9, 0.3310540860350354), (10, 0.3310540860350354), (11, 0.3310540860350354)]\n",
      "[(12, 0.39318346293947504), (13, 0.39318346293947504), (14, 0.39318346293947504), (15, 0.39318346293947504), (16, 0.2691478902490874), (17, 0.39318346293947504), (18, 0.39318346293947504)]\n",
      "[(0, 0.18035474530072435), (8, 0.18035474530072435), (19, 0.26347040375935565), (20, 0.5269408075187113), (21, 0.26347040375935565), (22, 0.26347040375935565), (23, 0.26347040375935565), (24, 0.26347040375935565), (25, 0.18035474530072435), (26, 0.26347040375935565), (27, 0.26347040375935565), (28, 0.26347040375935565), (29, 0.26347040375935565)]\n",
      "[(30, 0.5), (31, 0.5), (32, 0.5), (33, 0.5)]\n",
      "[(34, 0.4500498962785324), (35, 0.4500498962785324), (36, 0.4500498962785324), (37, 0.3080749612015952), (38, 0.3080749612015952), (39, 0.4500498962785324)]\n",
      "[(4, 0.175418584692144), (6, 0.24016036497261975), (25, 0.24016036497261975), (37, 0.24016036497261975), (38, 0.24016036497261975), (40, 0.350837169384288), (41, 0.350837169384288), (42, 0.350837169384288), (43, 0.350837169384288), (44, 0.350837169384288), (45, 0.350837169384288)]\n",
      "[(46, 1.0)]\n",
      "[(47, 0.4472135954999579), (48, 0.4472135954999579), (49, 0.4472135954999579), (50, 0.4472135954999579), (51, 0.4472135954999579)]\n",
      "[(1, 0.2046611868046407), (4, 0.1494891787609401), (16, 0.2046611868046407), (52, 0.2989783575218802), (53, 0.2989783575218802), (54, 0.2989783575218802), (55, 0.2989783575218802), (56, 0.2989783575218802), (57, 0.2989783575218802), (58, 0.2989783575218802), (59, 0.2989783575218802), (60, 0.2989783575218802), (61, 0.2989783575218802)]\n"
     ]
    }
   ],
   "source": [
    "# Creation of the TF-IDF scores for each word\n",
    "# Essentially calculates a value for each word based on it's frequency in each sentence \n",
    "## and also takes into account frequency of word in document overall\n",
    "\n",
    "dictionary = corpora.Dictionary(sentences)\n",
    "print(dictionary)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in sentences]\n",
    "tfidf = models.TfidfModel(doc_term_matrix)\n",
    "sentences_tfidf = tfidf[doc_term_matrix]\n",
    "for sent in sentences_tfidf:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4045484405709039, 0.366378392464812, 0.40398161136138316, 0.39120445759058003, 0.4078682168757249, 0.37040766661676466, 0.33697431194923594, 0.31537311193802886]\n",
      "optimal number of topics: 6\n"
     ]
    }
   ],
   "source": [
    "# Calculation of coherence values, which tells us the optimal number of topics to separate article into\n",
    "\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "for num_topics in range(2, 10): # The range indicates the numbers of topics we are considering; here we are considering between 2 and 9 topics\n",
    "    model = LsiModel(sentences_tfidf, num_topics=num_topics, id2word=dictionary)\n",
    "    model_list.append(model)\n",
    "    coherencemodel = CoherenceModel(model=model, texts=sentences, dictionary=dictionary)\n",
    "    coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "print(coherence_values)\n",
    "max_coherence = coherence_values.index(max(coherence_values))\n",
    "num_topics = 2 + max_coherence\n",
    "model = model_list[max_coherence]\n",
    "print(f'optimal number of topics: {num_topics}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.263*\"t\" + 0.263*\"il\" + 0.235*\"matin\" + 0.208*\"incendie\" + 0.200*\"heure\" + 0.200*\"5\" + 0.200*\"démarré\" + 0.200*\"secours\" + 0.200*\"déjà\" + 0.200*\"prévenir\"'), (1, '0.239*\"coma\" + 0.239*\"transférer\" + 0.239*\"ajouter\" + 0.239*\"Marseille\" + 0.232*\"il\" + 0.232*\"t\" + -0.191*\"origine\" + -0.188*\"Nîmes\" + -0.146*\"priori\" + -0.146*\"procureur\"'), (2, '-0.286*\"appartement\" + 0.243*\"origine\" + 0.235*\"feu\" + 0.235*\"immeuble\" + 0.235*\"étage\" + 0.235*\"ignore\" + 0.235*\"instant\" + 0.235*\"prendre\" + -0.161*\"personne\" + -0.161*\"Gard\"'), (3, '0.448*\"grave\" + 0.448*\"dénombre\" + 0.448*\"blessé\" + 0.448*\"également\" + 0.303*\"France\" + 0.144*\"Info\" + 0.144*\"enfant\" + 0.144*\"précise\" + 0.144*\"victime\" + 0.144*\"adulte\"'), (4, '-0.895*\"France\" + 0.187*\"Info\" + 0.187*\"précise\" + 0.187*\"enfant\" + 0.187*\"victime\" + 0.187*\"adulte\" + 0.076*\"dénombre\" + 0.076*\"grave\" + 0.076*\"également\" + 0.076*\"blessé\"'), (5, '-0.380*\"Info\" + -0.380*\"victime\" + -0.380*\"précise\" + -0.380*\"enfant\" + -0.380*\"adulte\" + -0.327*\"France\" + 0.208*\"grave\" + 0.208*\"dénombre\" + 0.208*\"blessé\" + 0.208*\"également\"')]\n",
      "\n",
      "[(0, 1.8987248960778664), (1, -1.4604938317813678), (2, -0.9734427754384644)] Cinq personnes sont mortes, et treize autres ont été blessées à Nîmes, dans le Gard, dans un incendie qui s'est déclenché vendredi 1er janvier au petit matin.\n",
      "[(0, 0.22053925371658237), (1, -0.9888033474091669), (2, 1.6561184541998684)] Le feu, dont on ignore l'origine pour l'instant, a pris au sixième et dernier étage d'un immeuble.\n",
      "[(0, 1.49967951720882), (1, -1.0774367414658732), (2, -2.2611627453875016)] \"A l'arrivée des pompiers, trois personnes étaient décédées dans un appartement, et deux autres dans un appartement voisin par intoxication\", a expliqué, sur i-Télé, le directeur de cabinet du préfet du Gard.\n",
      "[(3, 1.7936007796425586), (4, 0.30468365205031966), (5, 0.8307611663040635)] On dénombre également \"treize blessés, dont trois graves.\n",
      "[(0, 1.2681047078648622), (1, 1.4188621942924309), (2, 0.6630774409809027)] Une personne dans le coma a été transférée à Marseille\", a-t-il ajouté.\n",
      "[(0, 2.356155775860713), (1, 0.9812048869645829), (2, 0.22518166721222574)] Les secours ont été prévenus vers 5 heures du matin, mais \"l'incendie avait déjà bien démarré\", a-t-il expliqué.\n",
      "[(3, 0.30338557593800464), (4, -0.8951026397998952), (5, -0.32672382303104996)] France\n",
      "[(3, 0.7200911509287125), (4, 0.9369718039156227), (5, -1.8983025504437556)] Info précise que les victimes sont trois adultes et deux enfants.\n",
      "[(0, 1.1445606469371634), (1, -1.9257494885437403), (2, 1.48756539981413)] \"L'origine de l'incendie est indéterminée mais a priori accidentelle\", a déclaré le procureur adjoint de la République de Nîmes, cité par Europe 1.\n"
     ]
    }
   ],
   "source": [
    "print(model.print_topics(num_topics=num_topics)) # prints each topic, which is essentially a set of words and a number indicating their relation to the topic\n",
    "print()\n",
    "corpus_lsi = model[doc_term_matrix]\n",
    "\n",
    "# This loop prints each sentence along with the values indicating how much the sentence relates to each topic\n",
    "## Some sentences have no relation to a topic, which is why some sentences are missing a score for certain topics\n",
    "for score, text in zip(corpus_lsi, doc.sents):\n",
    "    print(score, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[(5, 2.356155775860713), (0, 1.8987248960778664), (2, 1.49967951720882), (4, 1.2681047078648622), (8, 1.1445606469371634), (1, 0.22053925371658237)]\n",
      "[(8, 1.9257494885437403), (0, 1.4604938317813678), (4, 1.4188621942924309), (2, 1.0774367414658732), (1, 0.9888033474091669), (5, 0.9812048869645829)]\n",
      "[(2, 2.2611627453875016), (1, 1.6561184541998684), (8, 1.48756539981413), (0, 0.9734427754384644), (4, 0.6630774409809027), (5, 0.22518166721222574)]\n",
      "[(3, 1.7936007796425586), (7, 0.7200911509287125), (6, 0.30338557593800464)]\n",
      "[(7, 0.9369718039156227), (6, 0.8951026397998952), (3, 0.30468365205031966)]\n",
      "[(7, 1.8983025504437556), (3, 0.8307611663040635), (6, 0.32672382303104996)]\n"
     ]
    }
   ],
   "source": [
    "# Sorts top sentence indices for each topic\n",
    "\n",
    "top_scores = [[] for i in range(num_topics)]\n",
    "\n",
    "# This loop helps us find the top scores\n",
    "for i, scores in enumerate(corpus_lsi): # i = sentence index in corpus\n",
    "    for j, score in scores: # j = topic index\n",
    "        top_scores[j].append((i, abs(score)))\n",
    "\n",
    "# This loop sorts the top scores in descending order for each topic\n",
    "for topic in top_scores:\n",
    "    topic.sort(reverse=True, key=lambda x: x[1])\n",
    "\n",
    "print()\n",
    "for v in top_scores:\n",
    "    print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "# This function takes a summary size (i.e. how many sentences we want in our summary)\n",
    "# Then it takes the top score from each topic and inserts the sentence index into our list 'top_sentences'\n",
    "\n",
    "def get_top_sentences(summary_size=5):\n",
    "    top_sentences = set()\n",
    "    count = 0\n",
    "    for i in range(summary_size): \n",
    "        for j in range(num_topics):\n",
    "            if i >= len(top_scores[j]):\n",
    "                continue\n",
    "            top_sentences.add(top_scores[j][i][0])\n",
    "            if len(top_sentences) == summary_size:\n",
    "                return sorted(top_sentences)\n",
    "            \n",
    "top_sentences = get_top_sentences()\n",
    "print(top_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "secours prévenir 5 heure matin incendie déjà démarré t il expliquer\n",
      "origine incendie indéterminer priori accidentel déclarer procureur adjoint république Nîmes citer Europe 1\n"
     ]
    }
   ],
   "source": [
    "# This block converts the top_sentences indicies back into their actual sentences and returns a summary that is 280 words or less\n",
    "# Thus all of the top_sentences may not always be used\n",
    "\n",
    "sents = list(doc.sents)\n",
    "longest_summary = ''\n",
    "for i in range(1, len(sents) + 1): # i = summary_size (i.e. number of sentences)\n",
    "    top_sentences = get_top_sentences(i)\n",
    "    summary = \"\"\n",
    "    for sent_idx in top_sentences: # joins words back together into strings and adds a new line between each sentence (this new line format was necessary to use the ROUGE evaluation)\n",
    "        #summary += sents[sent_idx].text + \"\\n\"\n",
    "        summary += ' '.join(word for word in sentences[sent_idx]) + \"\\n\"\n",
    "    if len(summary) > 280:\n",
    "        break\n",
    "    longest_summary = summary\n",
    "if longest_summary: # Omits las character in summary, which would be a new line ('\\n')\n",
    "    longest_summary = longest_summary[:-1]\n",
    "\n",
    "print(longest_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['personne', 'mourir', 'blesser', 'Nîmes', 'Gard', 'incendie', 'déclencher', 'vendredi', 'premier', 'janvier', 'petit', 'matin']]\n",
      "personne mourir blesser Nîmes Gard incendie déclencher vendredi premier janvier petit matin\n",
      "{'rougeL': Score(precision=0.7741935483870968, recall=0.13114754098360656, fmeasure=0.22429906542056074)}\n",
      "{'rougeL': Score(precision=0.06451612903225806, recall=0.14285714285714285, fmeasure=0.08888888888888889)}\n"
     ]
    }
   ],
   "source": [
    "# ROUGE Evaluation\n",
    "\n",
    "summ_sentences = []\n",
    "summ_cur_sentence = []\n",
    "\n",
    "# This loop takes the words in the reference summary (from the corpus) to remove stop words and stem the remaining keywords\n",
    "for sent in summ.sents: # For each sentence in the summary\n",
    "    for token in sent: # For each word in a sentence\n",
    "        if not token.text.lower() in STOP_WORDS and not token.is_punct:\n",
    "            summ_cur_sentence.append(token.lemma_)\n",
    "    summ_sentences.append(summ_cur_sentence)\n",
    "    summ_cur_sentence = []\n",
    "\n",
    "# Instantiate ROUGE evaluation\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
    "\n",
    "# joins elements in original article into a string from a list\n",
    "original_article = '\\n'.join([sent.text for sent in doc.sents])\n",
    "scores_original = scorer.score(original_article, longest_summary)\n",
    "\n",
    "# joins elements in reference summary into a string from a list\n",
    "#reference_summary = '\\n'.join([sent.text for sent in summ.sents])\n",
    "reference_summary = '\\n'.join([' '.join(sent) for sent in summ_sentences])\n",
    "print(summ_sentences)\n",
    "print(reference_summary)\n",
    "\n",
    "# The ROUGE score evaluates the reference summary against our generated summary (longest_summary)\n",
    "scores_reference = scorer.score(reference_summary, longest_summary)\n",
    "\n",
    "#print()\n",
    "#print(longest_summary)\n",
    "#print()\n",
    "#print(original_article)\n",
    "#print()\n",
    "#print(reference_summary)\n",
    "#print()\n",
    "print(scores_original)\n",
    "#print()\n",
    "print(scores_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
